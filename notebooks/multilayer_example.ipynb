{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Siamese Network Example\n",
    "\n",
    "This notebook shows you how to create and use a version of YOLOv8n (adaptable to any other yolo version). \n",
    "\n",
    "The code example requires a yolo dataset. Here we have a dataset of tiffs with 6 bands. Where the first three bands are the RGB bands of timestamp 1 and the last three bands are the RGB bands of timestamp 2. \n",
    "\n",
    "Please note that the dateset is tiny by design and is not able to produce any meaningful results. The purpose of this notebook is to show you how to define and use the model using additional bands as input features.\n",
    "\n",
    "Please note that this notebook is mainly for showing how to use the network in comparison to the normal ultralytics repositry."
   ],
   "id": "16ab93615f8d3ff2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using pretrained rgb weights as starting point for training\n",
    "\n",
    "this package allows you to use pretrained weights supplied by ultralytics.\n",
    "It will adjust the defined layers to adjust to the number of input channels.\n",
    "\n",
    "To do so you must pass the adjust_layers parameter. In the case of yolov8 we must only adjust the first layer. Checkout the config yamls, to figure out which layers to adjust. (Usually only the first layer, but eg. yolov9 requires the adjustment of layers 1 and 16)\n",
    "\n",
    "In addition you could adjust the config yamls, instead, but I would recommend to use the adjust_layers parameter, as it is more flexible and less error-prone."
   ],
   "id": "6edee01cd1a95f4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T12:59:10.571910Z",
     "start_time": "2024-09-03T12:58:20.598717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics_MB import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "model.train(data=\"./notebooks/datasets/siamese_test/data.yaml\",imgsz=124,epochs=1,batch=1,single_cls=True,channels=6,adjust_layers=[0])"
   ],
   "id": "8798b5625ee879e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.27 üöÄ Python-3.10.14 torch-2.3.0+cu121 CPU (Intel Core(TM) i7-8550U 1.80GHz)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=./notebooks/datasets/siamese_test/data.yaml, epochs=1, time=None, patience=100, batch=1, imgsz=124, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train78, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, channels=6, adjust_layers=[0], dual_stream=False, bands=[0], overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train78\n",
      "ch = 3\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics_MB.nn.modules.conv.Conv          [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics_MB.nn.modules.conv.Conv          [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics_MB.nn.modules.block.C2f          [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics_MB.nn.modules.conv.Conv          [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics_MB.nn.modules.block.C2f          [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics_MB.nn.modules.conv.Conv          [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics_MB.nn.modules.block.C2f          [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics_MB.nn.modules.conv.Conv          [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics_MB.nn.modules.block.C2f          [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics_MB.nn.modules.block.SPPF         [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics_MB.nn.modules.conv.Concat        [1]                           \n",
      " 12                  -1  1    148224  ultralytics_MB.nn.modules.block.C2f          [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics_MB.nn.modules.conv.Concat        [1]                           \n",
      " 15                  -1  1     37248  ultralytics_MB.nn.modules.block.C2f          [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics_MB.nn.modules.conv.Conv          [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics_MB.nn.modules.conv.Concat        [1]                           \n",
      " 18                  -1  1    123648  ultralytics_MB.nn.modules.block.C2f          [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics_MB.nn.modules.conv.Conv          [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics_MB.nn.modules.conv.Concat        [1]                           \n",
      " 21                  -1  1    493056  ultralytics_MB.nn.modules.block.C2f          [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752677  ultralytics_MB.nn.modules.head.Detect        [7, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING ‚ö†Ô∏è imgsz=[124] must be multiple of max stride 32, updating to [128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/clanger/Desktop/machinelearning/ultralytics_multiband_support/notebooks/datasets/siamese_test/train/labels.cache... 5 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<?, ?it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/clanger/Desktop/machinelearning/ultralytics_multiband_support/notebooks/datasets/siamese_test/val/labels.cache... 5 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train78/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 128 train, 128 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mruns/detect/train78\u001B[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      2.916      4.623      1.681          5        128: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5         37          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs completed in 0.002 hours.\n",
      "Optimizer stripped from runs/detect/train78/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train78/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train78/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.27 üöÄ Python-3.10.14 torch-2.3.0+cu121 CPU (Intel Core(TM) i7-8550U 1.80GHz)\n",
      "Model summary (fused): 168 layers, 3,007,445 parameters, 0 gradients, 8.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5         37          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 51.0ms preprocess, 3592.4ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001B[1mruns/detect/train78\u001B[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetMetrics' object has no attribute 'curves_results'. See valid attributes below.\n\n    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n    (mAP) of an object detection model.\n\n    Args:\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (tuple of str): A tuple of strings that represents the names of the classes.\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n\n    Methods:\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n        keys: Returns a list of keys for accessing the computed detection metrics.\n        mean_results: Returns a list of mean values for the computed detection metrics.\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n        fitness: Computes the fitness score based on the computed detection metrics.\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n        curves: TODO\n        curves_results: TODO\n    ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/IPython/core/formatters.py:711\u001B[0m, in \u001B[0;36mPlainTextFormatter.__call__\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    704\u001B[0m stream \u001B[38;5;241m=\u001B[39m StringIO()\n\u001B[1;32m    705\u001B[0m printer \u001B[38;5;241m=\u001B[39m pretty\u001B[38;5;241m.\u001B[39mRepresentationPrinter(stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnewline,\n\u001B[1;32m    707\u001B[0m     max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length,\n\u001B[1;32m    708\u001B[0m     singleton_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingleton_printers,\n\u001B[1;32m    709\u001B[0m     type_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_printers,\n\u001B[1;32m    710\u001B[0m     deferred_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeferred_printers)\n\u001B[0;32m--> 711\u001B[0m \u001B[43mprinter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m printer\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    713\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/IPython/lib/pretty.py:419\u001B[0m, in \u001B[0;36mRepresentationPrinter.pretty\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    408\u001B[0m                         \u001B[38;5;28;01mreturn\u001B[39;00m meth(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[1;32m    409\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    410\u001B[0m                     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\n\u001B[1;32m    411\u001B[0m                     \u001B[38;5;66;03m# check if cls defines __repr__\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    417\u001B[0m                     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(_safe_getattr(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__repr__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    418\u001B[0m                 ):\n\u001B[0;32m--> 419\u001B[0m                     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_repr_pprint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    421\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_pprint(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/IPython/lib/pretty.py:787\u001B[0m, in \u001B[0;36m_repr_pprint\u001B[0;34m(obj, p, cycle)\u001B[0m\n\u001B[1;32m    785\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001B[39;00m\n\u001B[1;32m    786\u001B[0m \u001B[38;5;66;03m# Find newlines and replace them with p.break_()\u001B[39;00m\n\u001B[0;32m--> 787\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    788\u001B[0m lines \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgroup():\n",
      "File \u001B[0;32m~/Desktop/machinelearning/ultralytics_multiband_support/ultralytics_MB/utils/__init__.py:160\u001B[0m, in \u001B[0;36mSimpleClass.__repr__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    159\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a machine-readable string representation of the object.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__str__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/machinelearning/ultralytics_multiband_support/ultralytics_MB/utils/__init__.py:148\u001B[0m, in \u001B[0;36mSimpleClass.__str__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    146\u001B[0m attr \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mdir\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 148\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(v) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m a\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    150\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, SimpleClass):\n\u001B[1;32m    151\u001B[0m             \u001B[38;5;66;03m# Display only the module and class name for subclasses\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/machinelearning/ultralytics_multiband_support/ultralytics_MB/utils/__init__.py:165\u001B[0m, in \u001B[0;36mSimpleClass.__getattr__\u001B[0;34m(self, attr)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001B[39;00m\n\u001B[1;32m    164\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m--> 165\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. See valid attributes below.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__doc__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DetMetrics' object has no attribute 'curves_results'. See valid attributes below.\n\n    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n    (mAP) of an object detection model.\n\n    Args:\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (tuple of str): A tuple of strings that represents the names of the classes.\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n\n    Methods:\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n        keys: Returns a list of keys for accessing the computed detection metrics.\n        mean_results: Returns a list of mean values for the computed detection metrics.\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n        fitness: Computes the fitness score based on the computed detection metrics.\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n        curves: TODO\n        curves_results: TODO\n    "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79627d17df2097fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
